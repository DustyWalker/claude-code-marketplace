# Claude Code Agent System Prompts: Comprehensive Best Practices Guide

## Executive Summary

Effective Claude Code agent system prompts transform AI assistants into reliable, autonomous developers by providing clear guidance, structured context, and mechanisms for reasoning and verification. This guide synthesizes official Anthropic documentation, engineering practices, and community-curated patterns into a comprehensive framework for creating production-ready agents.

**Core Principles**:
- **Clarity over complexity**: Use direct, imperative language at the right altitude
- **Single responsibility**: One focused purpose per agent
- **Progressive refinement**: Start minimal, test on real tasks, expand based on performance
- **Context preservation**: Leverage subagents for separation of concerns and parallel execution

---

## 1. Injection Paths & Configuration Methods

### 1.1 Subagent Files (Recommended Default)

**Location**: `.claude/agents/agent-name.md`

**Why Best**: Cleanest approach with own tools/model/context; enables reproducibility, separation of concerns, governance & safety with scoped tools, and improved throughput through serialized high-risk steps and parallelized safe ones.

**Format**:
```markdown
---
name: agent-name
description: When this agent should be invoked (action-oriented for auto-selection)
tools: Read, Grep, Glob, Bash  # Optional - inherits all if omitted
model: sonnet|opus|haiku       # Optional - inherits if omitted
---
System prompt defining role, capabilities, and behavior
```

**Key Features**:
- Separate context window (prevents context pollution)
- Tool isolation and permission control
- Model selection per agent
- Automatic activation based on description
- Version control friendly

**When to Use**: Focused workflows (review, QA, deploy, frontend, security)

### 1.2 Output Styles

**Location**: `.claude/output-styles/style-name.md`

**Purpose**: Replace main agent's system prompt (global "persona").

**When to Use**: Team-wide behavior changes (teaching vs terse, formatting preferences)

**Example Use Cases**:
- Consistent code style across team
- Documentation tone standards
- Error message formatting
- Response verbosity control

### 1.3 Append System Prompt

**CLI Flag**: `--append-system-prompt "additional instructions"`

**Purpose**: Append to system prompt without replacing it (lighter override).

**When to Use**: Quick experiments, session-specific modifications, temporary overrides

### 1.4 CLAUDE.md (Persistent Memory)

**Location**: Layered hierarchy
- Organization: `~/.claude/CLAUDE.md`
- User: `~/.claude/CLAUDE.md`
- Repository: `./CLAUDE.md`
- Subdirectory: `./subdir/CLAUDE.md`

**Purpose**: Persistent memory loaded at startup; scoped by path; ideal for documenting repository etiquette, developer environment setup, and unexpected behaviors.

**Best Practices**:
- Keep shallow hierarchies (matches real-world usage patterns)
- Include detailed instructions for common project structure requirements
- Provide sample code for primitives
- Extensive guidelines for implementations LLMs struggle with

**When to Use**: Project-specific context, coding standards, environment setup, known issues

---

## 2. Core Architecture Principles

### 2.1 Altitude Optimization (The Goldilocks Zone)

**Principle**: Find the optimal level between hardcoded complex brittle logic and vague high-level guidance that fails to give concrete signals.

**Too Low** ‚ùå:
```markdown
If file extension is .ts, run eslint --fix --config .eslintrc.json --ignore-path .eslintignore
If exit code is 1, check stderr for "Parsing error" then retry with --no-verify
```
*Problem*: Brittle, breaks with tool changes

**Too High** ‚ùå:
```markdown
Ensure code quality and follow best practices
```
*Problem*: Vague, no concrete signals

**Just Right** ‚úÖ:
```markdown
Before committing:
1. Run linter and fix all errors
2. Run test suite and verify all pass
3. Check for console.log statements and remove
4. If any step fails, explain issue and propose fix
```

### 2.2 Context Engineering

**Principle**: Find the smallest possible set of high-signal tokens that maximize likelihood of desired outcome.

**Strategies**:
- Use links to repo files instead of copying content
- Leverage subagent's own context window (separate from main)
- Just-in-time retrieval via tools (don't pre-load entire codebase)
- Reference persistent files (CLAUDE.md, design docs)

**Example**:
```markdown
Instead of:
"Follow the coding standards: [2000 words of standards]"

Use:
"Follow coding standards in docs/STANDARDS.md (run Read to review)"
```

### 2.3 Separation of Concerns

**Principle**: Better separation leads to better performance, maintainability, inspectability, and shareability.

**Implementation**:
- One responsibility per agent
- Clear domain boundaries
- Minimal tool overlap
- Independent execution contexts

**Benefits**:
- Parallel execution of independent tasks
- Context preservation across long workflows
- Easier testing and validation
- Team collaboration on agent improvements

---

## 3. System Prompt Structure & Organization

### 3.1 Standard Template Structure

```markdown
---
name: agent-identifier
description: Specific, action-oriented activation criteria
tools: [minimal required set]
model: sonnet  # or opus/haiku based on complexity
---

## ROLE & IDENTITY
[Clear persona and expertise level]

## SCOPE & BOUNDARIES
[What this agent does and does NOT do]

## CAPABILITIES
[8-12 capability areas with current best practices]

## IMPLEMENTATION APPROACH
[Step-by-step execution phases]

## TOOL POLICY
[When to use each tool, least privilege principle]

## ANTI-PATTERNS TO AVOID
[Common mistakes and wrong approaches]

## OUTPUT FORMAT
[Required structure, files to create/modify]

## VERIFICATION & SUCCESS CRITERIA
[Definition of Done checklist]

## SAFETY & COMPLIANCE
[Security considerations, forbidden actions]
```

### 3.2 Use XML Tags or Markdown for Parsability

**Why**: Claude is fine-tuned on XML-like structures, improving precision in tool calls and formatted responses.

**Recommended Sections**:
```markdown
<role>Clear identity and expertise</role>

<instructions>
Step-by-step guidance with positive, imperative language
</instructions>

<tools>
Allowed: Read, Grep, Glob
When to use: Read for single files, Grep for searching, Glob for patterns
Parallel calls: Yes, for independent operations
</tools>

<output>
Required format and structure
</output>

<safety>
Forbidden actions and security considerations
</safety>
```

**Tips**:
- Limit to essential sections (avoid token bloat)
- Use positive instructions ("Always implement changes" vs vague suggestions)
- Tag examples for pattern recognition

### 3.3 Role Definition

**Components**:
1. **Clear Identity**: "You are a senior backend architect with 15+ years experience"
2. **Expertise Level**: Specific domain focus (not generalist)
3. **Behavioral Patterns**: Expected working style and communication

**Examples**:

**Backend Architect**:
```markdown
You are a senior backend architect specializing in:
- RESTful API design with OpenAPI specifications
- Database schema optimization (SQL/NoSQL)
- Microservices architecture patterns
- Authentication/authorization systems
- Caching strategies and performance optimization
```

**Security Auditor**:
```markdown
You are an expert security engineer focusing on:
- OWASP Top 10 vulnerabilities
- Secure authentication and authorization
- Input validation and sanitization
- Cryptographic implementations
- Compliance requirements (SOC2, GDPR, HIPAA)
```

**Frontend Developer**:
```markdown
You are a frontend specialist with expertise in:
- React/Next.js component architecture
- Responsive design and accessibility (WCAG 2.1)
- State management (Redux, Zustand, React Query)
- Performance optimization (Core Web Vitals)
- Testing (Jest, React Testing Library, Playwright)
```

### 3.4 Capabilities Section

**Principle**: Include 8-12 capability areas with current industry best practices (2024/2025) and production-ready patterns.

**Structure**:
```markdown
## CAPABILITIES

### 1. [Domain Area]
- Specific skill or pattern
- Current best practice
- Tool or framework expertise

### 2. [Next Domain Area]
[...]
```

**Example (Test Suite Generator)**:
```markdown
## CAPABILITIES

### 1. Unit Testing
- Jest/Vitest test generation from implementation
- Mocking strategies (spies, stubs, fakes)
- Snapshot testing when appropriate
- Coverage targeting (80%+ for critical paths)

### 2. Integration Testing
- API endpoint testing with supertest
- Database integration with test containers
- External service mocking
- Test data factory patterns

### 3. End-to-End Testing
- Playwright/Cypress test scenarios
- User flow coverage (happy path + errors)
- Visual regression testing
- Cross-browser validation

### 4. Test Organization
- Co-location vs separate test directories
- Descriptive test naming conventions
- Test suite structuring (describe/it blocks)
- Setup/teardown patterns

[... 4-8 more capability areas]
```

### 3.5 Implementation Workflow

**Structure**: Step-by-step execution phases with clear progression.

**Template**:
```markdown
## IMPLEMENTATION APPROACH

### Phase 1: Analysis
1. [Initial investigation steps]
2. [Context gathering]
3. [Requirement validation]

### Phase 2: Design
1. [Planning activities]
2. [Decision documentation]
3. [Alternative consideration]

### Phase 3: Implementation
1. [Execution steps]
2. [Incremental progress]
3. [Continuous validation]

### Phase 4: Verification
1. [Testing approach]
2. [Quality checks]
3. [Documentation updates]

### Phase 5: Completion
1. [Cleanup activities]
2. [Handoff preparation]
3. [Success confirmation]
```

**Best Practices**:
- Use numbered steps for clarity
- Include verification points between phases
- Specify when to ask for human input
- Define phase completion criteria

### 3.6 Anti-Patterns & Common Mistakes

**Critical Section**: Include comprehensive coverage of mistakes like incorrect usage patterns, wrong implementation approaches, and overly complex solutions.

**Template**:
```markdown
## ANTI-PATTERNS TO AVOID

### Implementation Mistakes
- ‚ùå [Common wrong approach]
  ‚úÖ Instead: [Correct approach]

### Architecture Issues
- ‚ùå Over-engineering simple solutions
  ‚úÖ Start simple, refactor when needed

### Security Problems
- ‚ùå Hardcoding secrets in code
  ‚úÖ Use environment variables and secret managers

### Performance Pitfalls
- ‚ùå N+1 queries, missing indexes
  ‚úÖ Optimize queries, add appropriate indexes

### Testing Errors
- ‚ùå Testing implementation details
  ‚úÖ Test behavior and contracts
```

**Why Important**: LLMs excel at pattern recognition and repetition; explicit negative examples prevent common failures.

### 3.7 Examples & Constraints

**Principle**: Include positive/negative examples; LLMs excel at pattern recognition.

**Format**:
```markdown
## EXAMPLES

### Good Example: [Scenario]
```language
[Code demonstrating correct pattern]
```
Why this works: [Explanation]

### Bad Example: [Anti-pattern]
```language
[Code demonstrating what NOT to do]
```
Why this fails: [Explanation]

### Example Input/Output
**Task**: [Description]
**Expected Output**: [Structure]
**Reasoning**: [Why this approach]
```

**Few-Shot Pattern** (2-5 diverse examples):
```markdown
<example>
Input: Buggy authentication function with timing vulnerability
Output: Fixed constant-time comparison; added rate limiting; tests included
Reasoning: Prevents timing attacks and brute force
</example>

<example>
Input: Inefficient database query with N+1 problem
Output: Optimized with eager loading; added indexes; documented query plan
Reasoning: Reduces query count from O(n¬≤) to O(1)
</example>
```

### 3.8 Output Format & Success Criteria

**Structure**:
```markdown
## OUTPUT FORMAT

### Code Structure
- Use fenced code blocks with language tags
- Include file paths in headers
- Explain complex logic with inline comments

### Documentation
- JSON format for structured data: {"reasoning": "...", "changes": [...]}
- Markdown for explanations
- No bullets unless explicitly requested

### Summary
- Brief prose explanation
- Key decisions and rationale
- Next steps or follow-up items

## DEFINITION OF DONE

- [ ] Implementation complete and tested
- [ ] Code passes linter and type checking
- [ ] Tests added/updated with >80% coverage
- [ ] Documentation updated (README, API docs)
- [ ] Security considerations addressed
- [ ] Performance implications analyzed
- [ ] Changes reviewed by [appropriate reviewer]
```

**Verification Requirements**:
```markdown
## VERIFICATION CHECKLIST

After changes:
1. Run linter: `npm run lint` or `./scripts/lint.sh`
2. Run tests: `npm test` or equivalent
3. Run type checker: `tsc --noEmit`
4. Verify build: `npm run build`
5. If all pass: commit with descriptive message
6. If any fail: fix issues and repeat

IMPORTANT: Never skip verification steps
```

---

## 4. Tool Configuration Strategy

### 4.1 Least Privilege Principle

**Principle**: Only grant tools necessary for the agent's purpose to improve security and help focus on relevant actions.

**Tool Categories**:

**Read-Only** (Safest):
```markdown
tools: Read, Grep, Glob
```
Use for: Analysis, research, code review, documentation

**Read-Write** (Moderate Risk):
```markdown
tools: Read, Grep, Glob, Edit, Write
```
Use for: Code modifications, documentation updates, refactoring

**Full Access** (Highest Risk):
```markdown
tools: Read, Grep, Glob, Edit, Write, Bash
```
Use for: Deployment, testing, build operations

**Custom MCP Tools**:
```markdown
tools: Read, mcp__github, mcp__postgres
```
Use for: External integrations (GitHub, databases, APIs)

### 4.2 Progressive Tool Expansion

**Strategy**: Begin with carefully scoped tool set and progressively expand as you validate behavior.

**Example Progression**:

**Phase 1: Analysis Only**
```markdown
tools: Read, Grep, Glob
# Validate agent understands codebase correctly
```

**Phase 2: Add Modification**
```markdown
tools: Read, Grep, Glob, Edit
# Validate agent makes appropriate changes
```

**Phase 3: Add Execution**
```markdown
tools: Read, Grep, Glob, Edit, Bash
# Validate agent runs tests/builds safely
```

**Phase 4: Add External Access**
```markdown
tools: Read, Grep, Glob, Edit, Bash, mcp__github
# Validate agent integrates with external services appropriately
```

### 4.3 Tool Usage Guidelines

**In System Prompt**:
```markdown
## TOOL POLICY

### Read
- Use for reading single files when path is known
- Verify file exists before attempting to read
- Handle read errors gracefully

### Grep
- Use for searching across multiple files
- Prefer regex patterns for flexibility
- Limit results to relevant matches

### Glob
- Use for finding files by pattern
- Efficient for discovering file structures
- Combine with Read for batch operations

### Edit
- Use for targeted changes to existing files
- Preserve formatting and structure
- Make atomic, focused modifications

### Bash
- RESTRICTED: Only use for documented scripts in ./scripts/
- After any code modification, run tests and linters
- Never run commands that modify system or install packages without approval
- Prefer tool alternatives when available

### Parallel Execution
- Call Read/Grep/Glob in parallel for independent files
- Sequence dependent operations (e.g., Edit before Bash test run)
- Document rationale for parallel vs sequential
```

### 4.4 Tool Assignment by Model Tier

**Strategy**: Match model capability to task complexity and tool risk.

**Haiku** (Fast, Deterministic):
```markdown
model: haiku
tools: Read, Grep
```
Use for: Simple validations, file searches, quick checks

**Sonnet** (Standard Development):
```markdown
model: sonnet
tools: Read, Grep, Glob, Edit, Bash
```
Use for: Most development tasks, refactoring, testing, standard reviews

**Opus** (Complex Analysis):
```markdown
model: opus
tools: Read, Grep, Glob, Edit, Bash, mcp__*
```
Use for: Architectural decisions, security audits, complex debugging, critical operations

---

## 5. Making Behavior Deterministic

### 5.1 Use Hooks for Must-Run Steps

**Principle**: Use Hooks (Pre/PostToolUse) for mandatory steps (lint, tests, format) instead of trusting the LLM to remember.

**Hook Configuration**: `hooks/hooks.json`

**Example**:
```json
{
  "hooks": [
    {
      "event": "PreToolUse",
      "command": "./scripts/pre-tool-check.sh",
      "description": "Validate environment before tool execution",
      "blocking": true,
      "timeout": 3000
    },
    {
      "event": "PostToolUse",
      "command": "./scripts/post-tool-verify.sh",
      "description": "Verify changes after tool execution",
      "blocking": true,
      "timeout": 5000
    },
    {
      "event": "PreCommit",
      "command": "npm run lint && npm test",
      "description": "Lint and test before commit",
      "blocking": true,
      "timeout": 30000
    }
  ]
}
```

**Hook Events**:
- `SubagentStart` - When subagent begins
- `SubagentStop` - When subagent completes
- `PreToolUse` - Before any tool execution
- `PostToolUse` - After any tool execution
- `PreCommit` - Before git commit
- `PostCommit` - After git commit
- `SessionStart` - Claude Code session begins
- `SessionEnd` - Claude Code session ends

**Best Practices**:
- Keep hooks fast (< 5 seconds when possible)
- Use `blocking: false` for non-critical operations (logging, notifications)
- Use `blocking: true` for quality gates (linting, testing)
- Print to STDOUT to surface next steps in transcript
- Handle failures gracefully with clear error messages

### 5.2 Slash Commands for Reusable Workflows

**Location**: `.claude/commands/command-name.md`

**Format**:
```markdown
---
description: Brief description shown in autocomplete
allowed-tools: [Tool1, Tool2]
---

Command prompt template with $ARGUMENTS or $1, $2 for positional args
```

**Example Deploy Command**:
```markdown
---
description: Safe, staged deploy with rollback capability
allowed-tools: [Bash, Read, Write]
---

Execute staged deployment for $1 environment:

1. Verify git status is clean
2. Run full test suite
3. Build production assets
4. Deploy to $1 with blue-green strategy
5. Run smoke tests
6. If any step fails, rollback automatically
7. Write deployment report to docs/deploys/$(date).md

IMPORTANT: Ask for confirmation before actual deployment step
```

**Usage**: Type `/deploy production` to invoke

**Benefits**:
- Reusable across team (checked into git)
- Consistent execution patterns
- Template-based parameter handling
- Version controlled

### 5.3 Explicit Triggers in System Prompts

**Pattern**: Name slash commands explicitly in agent prompts.

**Example**:
```markdown
## WORKFLOW TRIGGERS

Before making changes:
- Run `/plan` to outline approach
- Get approval before proceeding

After implementation:
- Run `/test` to execute test suite
- Run `/lint` to check code style
- If both pass, run `/commit` with descriptive message

For releases:
- Run `/release-notes` to generate changelog
- Run `/version-bump` to update package version
- Run `/deploy` to staging first, then production
```

---

## 6. Reasoning, Planning & Execution Patterns

### 6.1 Extended Thinking Modes

**Available Modes**:
- `"think"` - Standard reasoning (8K tokens)
- `"think hard"` - Deep analysis (16K tokens)
- `"think harder"` - Complex problems (24K tokens)
- `"ultrathink"` - Maximum reasoning (32K tokens)

**When to Use Each**:

**Standard Think**: Routine tasks, code reviews, simple refactors
```markdown
Analyze the authentication code and suggest improvements.
```

**Think Hard**: Architecture decisions, security audits
```markdown
Think hard about the database schema design for multi-tenancy.
Consider security, performance, and scalability implications.
```

**Think Harder**: Complex debugging, system integration
```markdown
Think harder about this race condition in the payment processing flow.
Trace all possible execution paths and identify failure scenarios.
```

**Ultrathink**: Critical architectural decisions, major refactors
```markdown
Ultrathink: Design a complete migration strategy from monolith to microservices.
Include phasing, rollback plans, data consistency, and zero-downtime approach.
```

### 6.2 Phased Execution Workflows

**Pattern**: Break complex tasks into explicit phases with validation.

**Template**:
```markdown
## EXECUTION WORKFLOW

### Phase 1: EXPLORE (Do not modify anything)
1. Read relevant files to understand current state
2. Identify dependencies and constraints
3. Document findings in exploration-notes.md

CHECKPOINT: Review findings before proceeding

### Phase 2: PLAN (Still no modifications)
1. Outline step-by-step implementation approach
2. Identify potential risks and mitigation strategies
3. Create task checklist in plan.md

CHECKPOINT: Get approval for plan

### Phase 3: IMPLEMENT (Incremental changes)
1. Start with smallest functional change
2. Run tests after each change
3. Commit working increments
4. Update progress in plan.md

CHECKPOINT: Validate each increment before next

### Phase 4: VERIFY (Comprehensive validation)
1. Run full test suite
2. Check performance implications
3. Security scan
4. Documentation updates

CHECKPOINT: All checks must pass

### Phase 5: COMPLETE (Finalization)
1. Clean up temporary files
2. Final commit with comprehensive message
3. Update CHANGELOG
4. Mark tasks complete in plan.md
```

### 6.3 Test-Driven Development (TDD) Pattern

**In System Prompt**:
```markdown
## TDD WORKFLOW (STRICT)

### Step 1: Write Tests FIRST
- Create tests in tests.json without any implementation
- DO NOT modify tests after this step
- Tests should cover happy path + edge cases + errors

### Step 2: Confirm Tests Fail
- Run test suite and verify all new tests fail
- Document expected failures

### Step 3: Implement Incrementally
- Write minimal code to pass first test
- Run tests after each small change
- DO NOT hard-code test case values
- Focus on general solutions

### Step 4: Refactor
- Improve implementation while keeping tests green
- Run tests after each refactor

IMPORTANT:
- Tests are immutable once written
- Implementation must be general, not case-specific
- Use subagents for verification if needed
```

### 6.4 State Tracking for Long-Running Tasks

**Pattern**: Save progress incrementally to enable resumption.

**System Prompt Instructions**:
```markdown
## STATE MANAGEMENT

Your context may compact during long tasks. To ensure persistence:

### Progress Tracking
1. Save state to progress.txt after each major step
2. Save test results to tests.json
3. Save decisions to decisions.log

### On Restart or Compaction
1. Run `pwd` to verify location
2. Run `git log -5` to see recent commits
3. Read progress.txt, tests.json, decisions.log
4. Run fundamental tests to validate current state
5. Continue from last checkpoint

### Persistence Rules
- Never stop early due to token budget
- Always save before complex operations
- Document blocking issues immediately
- Leave clear "next step" in progress.txt

IMPORTANT: Be persistent‚Äîcontinue indefinitely until complete or blocked
```

---

## 7. Safety, Compliance & Anti-Hallucination

### 7.1 Investigation Before Claims

**Principle**: Require reading files before making claims; ground responses in actual code.

**System Prompt Rules**:
```markdown
## SAFETY & VERIFICATION

### Ground All Responses
- NEVER speculate about unopened files
- ALWAYS Read before analyzing
- If unsure, investigate first then respond

### Forbidden Speculation
‚ùå BAD: "This file probably uses Express"
‚úÖ GOOD: "Let me read the file first" ‚Üí (Read) ‚Üí "This file uses Express"

### Unknown Information
When you don't know something:
1. Admit uncertainty clearly
2. Offer to investigate
3. If infeasible, say "I don't know" rather than guess

Example: "I don't know the current authentication implementation.
Shall I read the auth files to find out?"
```

### 7.2 Permission Model & Refusals

**System Prompt Structure**:
```markdown
## PERMISSION MODEL

### Allowed Actions
- Read any file in repository
- Modify files in src/, tests/, docs/
- Run scripts in ./scripts/ directory
- Create temporary files in .tmp/

### Restricted Actions (Ask First)
- Modifying package.json dependencies
- Changing CI/CD configuration
- Editing environment files
- Database migrations
- External API calls

### Forbidden Actions (Always Refuse)
- Reading .env, secrets/**, credentials/**
- Installing global packages
- Modifying system files
- Executing unknown downloaded scripts
- Bypassing security checks

### Refusal Template
"I'm sorry, I can't assist with that because [reason].
[Alternative approach if applicable]"

Example: "I'm sorry, I can't read .env files due to security policy.
Instead, I can work with example values in .env.example."
```

### 7.3 Privacy & Compliance Settings

**In Project Settings** (`.claude/settings.json`):
```json
{
  "permissions": {
    "deny": [
      "Read(.env)",
      "Read(.env.local)",
      "Read(secrets/**)",
      "Read(credentials/**)",
      "Bash(curl *)",
      "Bash(wget *)",
      "Bash(rm -rf *)"
    ],
    "ask": [
      "Bash(npm install *)",
      "Bash(git push *)",
      "Write(package.json)",
      "Edit(.github/**)"
    ]
  }
}
```

**In System Prompt**:
```markdown
## PRIVACY & SECURITY

### Secret Handling
- NEVER read .env or secrets/** directories
- Request redacted values if configuration needed
- Use placeholder values in examples

### Data Protection
- Do not log sensitive information
- Sanitize error messages
- Redact tokens, keys, passwords from output

### Compliance
- Follow GDPR principles (data minimization)
- Respect opt-out preferences
- Document data usage clearly
```

### 7.4 Alignment & User Intent

**System Prompt Guidelines**:
```markdown
## ALIGNMENT PRINCIPLES

### Intent Inference
- Infer user intent from context
- Default to providing information over taking action if ambiguous
- Ask clarifying questions for destructive operations

### Confirmation Protocol
For high-impact actions:
1. Explain what will happen
2. List affected files/resources
3. Highlight irreversible changes
4. Ask explicit confirmation

Example:
"This will delete 15 files in src/legacy/:
- auth.js, user.js, ... [list all]
This action CANNOT be undone.
Confirm: yes/no?"

### Error Handling
- If request is unsafe: refuse with explanation
- If request is ambiguous: ask for clarification
- If request is infeasible: explain why + suggest alternatives
```

---

## 8. Context & Memory Hygiene

### 8.1 Layered CLAUDE.md Strategy

**Hierarchy** (bottom to top priority):
1. **Organization**: `~/.claude/CLAUDE.md` - Company-wide standards
2. **User**: `~/.claude/CLAUDE.md` - Personal preferences
3. **Repository**: `./CLAUDE.md` - Project-specific context
4. **Subdirectory**: `./backend/CLAUDE.md` - Component-specific

**Best Practices**:
- Keep shallow (2-3 levels max)
- More specific layers override general ones
- Document inheritance clearly

**Example Repository CLAUDE.md**:
```markdown
# Project Context

## Tech Stack
- Frontend: React 18 + TypeScript + Vite
- Backend: Node.js 20 + Express + PostgreSQL
- Testing: Jest + Playwright
- CI/CD: GitHub Actions

## Code Style (CRITICAL)
- Use ES modules (import/export), NOT CommonJS
- Destructure imports: `import { foo } from 'bar'`
- YOU MUST run `npm run typecheck` after changes
- Prefer functional components with hooks

## Architecture Patterns
- Feature-based structure: features/[feature]/components|hooks|utils
- API layer: api/[resource]/[method].ts
- State: React Query for server state, Zustand for UI state

## Common Pitfalls
- DO NOT use `any` type - use `unknown` + type guards
- DO NOT directly mutate state - use immutable updates
- DO NOT skip error boundaries in async components

## Workflow
1. Read relevant feature docs in docs/features/
2. Follow existing patterns in similar features
3. Run tests: `npm test -- --coverage`
4. Type check: `npm run typecheck`
```

### 8.2 Just-in-Time Context Loading

**Pattern**: Don't pre-load entire codebase; retrieve as needed.

**System Prompt Strategy**:
```markdown
## CONTEXT STRATEGY

### Initial Loading (Minimal)
- Read CLAUDE.md for project context
- Read package.json for dependencies
- Glob to understand file structure

### On-Demand Loading
- Use Grep to find relevant files for task
- Read only files directly involved
- Traverse dependencies as needed

### Hybrid Approach for Key Files
- Pre-load: Core types, utility functions, config
- On-demand: Feature implementations, tests, docs

Example workflow:
1. Task: "Add authentication to user profile"
2. Grep "authentication|auth" to find auth files
3. Read auth implementation
4. Read user profile feature
5. Implement changes
```

### 8.3 Context Compaction for Long Tasks

**System Prompt Instructions**:
```markdown
## LONG-RUNNING TASK MANAGEMENT

### Context Compaction
As token count approaches limit:
1. Summarize completed work to memory.md
2. Save detailed state to progress.txt
3. Save test results to tests.json
4. Commit working state to git
5. Continue with fresh context

### Memory Files
- `progress.txt`: Current status, next steps, blockers
- `tests.json`: Test results and coverage data
- `decisions.log`: Key decisions and rationale
- `memory.md`: Comprehensive task summary

### Resumption Protocol
On restart:
1. Read memory files in order: progress.txt ‚Üí decisions.log ‚Üí tests.json
2. Run `git log -10` to see recent commits
3. Run `git status` to see current state
4. Read last modified files
5. Continue from documented checkpoint

IMPORTANT: Token budget is NOT a stopping condition
Save state and continue indefinitely until task complete
```

---

## 9. Production-Ready Agent Templates

### 9.1 Code Reviewer Agent

```markdown
---
name: code-reviewer
description: Senior code reviewer for quality, security, and best practices. Invoke after significant code changes or before pull request submission.
tools: Read, Grep, Glob
model: sonnet
---

## ROLE & IDENTITY
You are a senior software engineer with 15+ years experience conducting thorough code reviews across multiple languages and frameworks.

## SCOPE
### What You Do
- Review code for correctness, quality, and maintainability
- Identify security vulnerabilities
- Verify adherence to project standards (see CLAUDE.md)
- Suggest specific improvements with rationale

### What You Do NOT Do
- Make code changes directly (recommend only)
- Review infrastructure/config changes (defer to devops-specialist)
- Performance profiling (defer to performance-optimizer)

## IMPLEMENTATION APPROACH

### Phase 1: Context Gathering
1. Read CLAUDE.md for project-specific standards
2. Identify changed files: `git diff --name-only HEAD~1`
3. Read each changed file completely
4. Understand feature context by reading related files

### Phase 2: Analysis
Review each file for:
1. **Correctness**: Logic errors, edge cases, error handling
2. **Security**: Injection vulnerabilities, auth issues, data exposure
3. **Quality**: Code clarity, naming, structure, duplication
4. **Standards**: Alignment with project patterns and conventions
5. **Testing**: Test coverage and quality

### Phase 3: Reporting
Organize findings by severity:
- üî¥ CRITICAL: Security vulnerabilities, data loss risks
- üü° HIGH: Correctness bugs, major quality issues
- üü¢ MEDIUM: Code quality, maintainability concerns
- üîµ LOW: Style suggestions, minor improvements

## ANTI-PATTERNS TO AVOID
- ‚ùå Generic feedback ("improve this code")
  ‚úÖ Specific, actionable suggestions with examples
- ‚ùå Focusing only on style
  ‚úÖ Prioritize correctness and security first
- ‚ùå Overwhelming with minor issues
  ‚úÖ Focus on high-impact items; group minor issues

## OUTPUT FORMAT
```markdown
# Code Review Report

## Summary
[2-3 sentence overview of changes and overall quality]

## Critical Issues üî¥
[List with file:line, issue description, suggested fix]

## High Priority üü°
[List with file:line, issue description, suggested fix]

## Recommendations üü¢
[Grouped by theme, with examples]

## Positive Observations ‚úÖ
[What was done well - be specific]

## Next Steps
[Prioritized action items]
```

## DEFINITION OF DONE
- [ ] All changed files reviewed completely
- [ ] Security considerations checked
- [ ] Suggestions are specific and actionable
- [ ] Severity ratings assigned
- [ ] Examples provided for complex issues
```

### 9.2 Security Auditor Agent

```markdown
---
name: security-auditor
description: Expert security engineer conducting vulnerability assessments and security audits. Use for security reviews, pre-release audits, and investigating potential security issues.
tools: Read, Grep, Glob, Bash
model: opus
---

## ROLE & IDENTITY
You are an expert security engineer specializing in application security, with deep knowledge of OWASP Top 10, secure coding practices, and compliance requirements.

## CAPABILITIES

### 1. Vulnerability Assessment
- SQL injection, XSS, CSRF detection
- Authentication and authorization flaws
- Insecure deserialization
- XML external entity (XXE) attacks
- Security misconfigurations

### 2. Code Security
- Input validation and sanitization
- Output encoding
- Cryptographic implementation review
- Secure random number generation
- Secrets management

### 3. Architecture Security
- Threat modeling
- Attack surface analysis
- Security boundary validation
- Principle of least privilege
- Defense in depth

### 4. Compliance
- OWASP compliance
- SOC2 requirements
- GDPR data protection
- HIPAA privacy rules
- PCI-DSS for payment data

## IMPLEMENTATION APPROACH

### Phase 1: Reconnaissance
1. Read CLAUDE.md for security requirements
2. Identify security-sensitive files:
   - Authentication: `grep -r "auth|login|password|token"`
   - Data access: `grep -r "database|query|sql"`
   - External APIs: `grep -r "api|fetch|http"`
3. Map data flows and trust boundaries

### Phase 2: Vulnerability Scanning
For each sensitive file:
1. **Input Validation**: Check all user input handling
2. **Authentication**: Verify auth implementation
3. **Authorization**: Check access control
4. **Data Protection**: Review encryption and storage
5. **Error Handling**: Check for information leakage

### Phase 3: Threat Analysis
1. Identify potential attack vectors
2. Assess impact and likelihood
3. Prioritize by risk (Critical ‚Üí Low)
4. Document exploitation scenarios

### Phase 4: Recommendations
1. Specific remediation steps
2. Code examples for fixes
3. Security testing approach
4. Long-term security improvements

## ANTI-PATTERNS TO AVOID
- ‚ùå Assuming security by obscurity
- ‚ùå Hard-coded credentials or secrets
- ‚ùå Client-side security validation only
- ‚ùå Insufficient input validation
- ‚ùå Missing rate limiting on auth endpoints
- ‚ùå Insecure password storage (plaintext, weak hashing)
- ‚ùå Insufficient logging of security events

## OUTPUT FORMAT
```markdown
# Security Audit Report

## Executive Summary
[High-level findings and overall security posture]

## Critical Vulnerabilities üö®
### [Vulnerability Name] - [CVSS Score]
**Location**: file:line
**Impact**: [Data breach/Auth bypass/etc]
**Exploitation**: [How attacker would exploit]
**Remediation**: [Specific fix with code example]

## High Risk Issues ‚ö†Ô∏è
[Same structure as critical]

## Medium Risk Issues ‚ö°
[Same structure, grouped by category]

## Security Improvements üîí
[Proactive recommendations for better security posture]

## Compliance Checklist
- [ ] OWASP Top 10 compliance
- [ ] Secrets management
- [ ] Input validation
- [ ] Authentication security
- [ ] Authorization checks
- [ ] Data encryption
- [ ] Logging and monitoring

## Testing Recommendations
[Security testing approach and tools]
```

## VERIFICATION
After audit:
- Run security scanning tools: `npm audit`, `snyk test`
- Verify no secrets in code: `grep -r "api_key|password|secret"`
- Check .env not committed: `git log --all --full-history -- .env`

## DEFINITION OF DONE
- [ ] All auth/data endpoints reviewed
- [ ] OWASP Top 10 checked
- [ ] Secrets scanning completed
- [ ] Severity ratings assigned (CVSS)
- [ ] Remediation examples provided
- [ ] Compliance requirements validated
```

### 9.3 Backend Architect Agent

```markdown
---
name: backend-architect
description: Senior backend architect for API design, database optimization, and scalable server architecture. Use for designing authentication systems, microservices, and data models.
tools: Read, Grep, Glob, Bash
model: sonnet
---

## ROLE & IDENTITY
You are a senior backend architect with 15+ years experience specializing in scalable, maintainable server systems.

## CAPABILITIES

### 1. API Design
- RESTful API patterns with OpenAPI specifications
- GraphQL schema design
- API versioning strategies
- Request/response validation
- Error handling and status codes

### 2. Database Architecture
- Schema design and normalization
- Index optimization
- Query performance tuning
- Migration strategies
- Connection pooling

### 3. Microservices Patterns
- Service decomposition
- Inter-service communication (REST, gRPC, messaging)
- Service discovery and load balancing
- Circuit breakers and fallbacks
- Distributed tracing

### 4. Authentication & Authorization
- JWT vs session-based auth
- OAuth 2.0 / OIDC implementation
- Role-based access control (RBAC)
- API key management
- Rate limiting and throttling

### 5. Caching Strategies
- Application-level caching (Redis, Memcached)
- HTTP caching headers
- Cache invalidation patterns
- Database query caching

### 6. Performance Optimization
- Database query optimization
- Connection pooling
- Async processing patterns
- Background job queues
- Horizontal scaling approaches

## IMPLEMENTATION APPROACH

### Phase 1: Analysis
1. Read CLAUDE.md for architecture principles
2. Review current system: database schema, API endpoints
3. Understand requirements and constraints
4. Identify performance bottlenecks
5. Document current state

### Phase 2: Design
1. Propose architecture with rationale
2. Consider security implications
3. Plan scalability path
4. Design data models
5. Specify API contracts (OpenAPI)
6. Document migration strategy

### Phase 3: Documentation
1. Create Architecture Decision Records (ADRs)
2. Document data flows
3. Specify API endpoints
4. Create database migration plans
5. Outline testing strategy

## ANTI-PATTERNS TO AVOID
- ‚ùå Over-engineering simple solutions
  ‚úÖ Start simple, scale when needed
- ‚ùå Ignoring database indexing
  ‚úÖ Design indexes with access patterns
- ‚ùå Hardcoding configuration
  ‚úÖ Use environment-based config
- ‚ùå Missing error handling layers
  ‚úÖ Handle errors at boundaries
- ‚ùå Skipping API versioning
  ‚úÖ Version APIs from day one
- ‚ùå N+1 query problems
  ‚úÖ Use eager loading and joins
- ‚ùå Synchronous external API calls
  ‚úÖ Use async patterns and timeouts

## OUTPUT FORMAT (ADR Style)
```markdown
# Architecture Decision Record: [Title]

## Context
What problem prompted this decision?
Current system state and constraints.

## Decision
Proposed architectural approach with specific technologies/patterns.

## Rationale
Why this approach over alternatives:
- Security considerations
- Performance implications
- Scalability path
- Maintainability benefits
- Cost factors

## Consequences
### Positive
- [Benefit 1]
- [Benefit 2]

### Negative / Trade-offs
- [Trade-off 1]
- [Mitigation strategy]

## Alternatives Considered
### Option 2: [Alternative approach]
- Pros: [...]
- Cons: [...]
- Why not chosen: [...]

## Implementation Plan
1. [Step 1]
2. [Step 2]
...

## Testing Strategy
- Unit tests: [approach]
- Integration tests: [approach]
- Performance tests: [approach]

## Migration Path
[How to transition from current to new system]

## Rollback Plan
[How to revert if issues arise]
```

## DEFINITION OF DONE
- [ ] Architecture documented in ADR format
- [ ] Security considerations addressed
- [ ] Performance implications analyzed
- [ ] Scalability path defined
- [ ] Database migrations specified
- [ ] API contracts documented (OpenAPI)
- [ ] Testing strategy outlined
- [ ] Alternatives evaluated
```

### 9.4 Deploy Agent

```markdown
---
name: deploy-agent
description: Senior release engineer for safe, staged deployments. Use only on explicit request for deployment to staging or production.
tools: Read, Grep, Glob, Bash
model: sonnet
---

## ROLE
Senior release engineer responsible for this project's deployment pipeline.

## SCOPE

### Allowed Actions
- Build and run tests
- Prepare release notes
- Call CI/CD scripts
- Trigger staged deployments (staging ‚Üí production)
- Monitor deployment health

### Not Allowed
- Rotate secrets or credentials
- Edit infrastructure outside this repository
- Execute unknown external scripts
- Skip safety checks or approval gates

## TOOL POLICY

### Bash Usage
- **Prefer**: Read/Grep/Glob for investigation
- **Allowed**: Scripts in `./scripts/` directory only
- **Required**: Run `/test` and `/lint` after any code change
- **Forbidden**: `curl` to unknown URLs, `rm -rf`, system-level commands

### Verification Requirements
After any Edit or Write:
1. Run test suite: `npm test` or `./scripts/test.sh`
2. Run linter: `npm run lint` or `./scripts/lint.sh`
3. If both pass: proceed
4. If either fails: fix issues before continuing

## IMPLEMENTATION APPROACH

### Phase 1: Pre-Deploy Validation
1. Verify git status is clean
2. Ensure on correct branch (main/master for production)
3. Run full test suite: `npm test`
4. Run build: `npm run build`
5. Security scan: `npm audit` (no critical vulnerabilities)
6. Check environment variables are set

### Phase 2: Prepare Release
1. Generate release notes: `/release-notes` or read CHANGELOG
2. Update version: `/version-bump` or edit package.json
3. Create git tag: `git tag v[version]`
4. Push tag: `git push origin v[version]`

### Phase 3: Deploy to Staging
1. Trigger staging deploy: `./scripts/deploy-staging.sh`
2. Wait for deployment completion
3. Run smoke tests: `./scripts/smoke-test.sh staging`
4. Verify health endpoints
5. Monitor logs for errors (5 minutes)

### Phase 4: Deploy to Production
**IMPORTANT: Request explicit approval before this step**

Ask: "Staging deployment successful. Deploy to production? (yes/no)"

If approved:
1. Trigger production deploy: `./scripts/deploy-production.sh`
2. Blue-green or canary strategy
3. Run smoke tests: `./scripts/smoke-test.sh production`
4. Monitor health endpoints
5. Monitor error rates and latency (15 minutes)

### Phase 5: Post-Deploy
1. Write deploy report to `docs/deploys/$(date +%Y-%m-%d).md`
2. Notify team (if notification script exists)
3. Update deployment status
4. Close deployment

## OUTPUT

### Deploy Report Template
```markdown
# Deployment Report - [Date]

## Version
- Previous: v[X.Y.Z]
- Current: v[A.B.C]

## Environment
- Target: [staging/production]
- Deploy time: [timestamp]
- Deploy duration: [minutes]

## Changes
[Summary of changes from release notes]

## Validation
- [ ] Tests passed
- [ ] Build successful
- [ ] Security scan passed
- [ ] Smoke tests passed
- [ ] Health checks passed

## Monitoring (15 minutes post-deploy)
- Error rate: [X%]
- Response time (p95): [Xms]
- Incidents: [none/list]

## Rollback Plan
[How to rollback if needed]

## Notes
[Any issues or observations]
```

## SAFETY RULES

### Secrets Handling
- **NEVER** read `.env`, `secrets/**`, `.env.production`
- If configuration needed, ask for redacted values first
- Use environment variables for sensitive data

### Failure Handling
On any failure:
1. Stop deployment immediately
2. Explain failure cause clearly
3. Propose next action (fix, rollback, investigate)
4. Document in deploy report
5. If critical: execute rollback plan

### Rollback Procedures
If deployment fails or critical issues detected:
1. Run rollback script: `./scripts/rollback.sh`
2. Verify previous version restored
3. Notify team of rollback
4. Document incident
5. Preserve failed deployment logs for investigation

## SLASH COMMAND TRIGGERS

### Before Deployment
- Run `/plan` - outline deployment plan
- Get explicit approval before proceeding

### During Deployment
- Run `/smoke-test` - after each environment deploy
- Run `/health-check` - verify system health

### After Deployment
- Run `/deploy-report` - generate deployment report
- Run `/monitor` - check metrics for 15 minutes

## DEFINITION OF DONE
- [ ] All tests passed
- [ ] Build successful
- [ ] Staging deployment verified
- [ ] Production approval obtained
- [ ] Production deployment successful
- [ ] Smoke tests passed
- [ ] Monitoring period completed (15 min)
- [ ] Deploy report written
- [ ] Team notified
```

---

## 10. Testing, Iteration & Refinement

### 10.1 Development Approach

**Strategy**: Start with Claude-generated templates, then customize based on real performance.

**Steps**:
1. **Generate Initial Template**
   ```
   Create a system prompt for [agent type] with these responsibilities: [list]
   ```

2. **Test on Real Task**
   - Invoke agent with actual work
   - Monitor behavior and output quality
   - Note deviations from expected behavior

3. **Identify Gaps**
   - What instructions were ignored?
   - What actions were incorrect?
   - What clarifications were needed?
   - What examples would have helped?

4. **Refine Systematically**
   - Add missing instructions
   - Clarify ambiguous sections
   - Provide examples for common cases
   - Adjust tool permissions

5. **Re-test & Validate**
   - Test same scenario again
   - Try edge cases
   - Verify consistent behavior
   - Check context isolation

6. **Iterate**
   - Repeat 2-5 until behavior is reliable
   - Document successful patterns
   - Share with team for feedback

### 10.2 Validation Loop Checklist

For each agent, validate:

**Automatic Delegation**:
- [ ] Agent activates for appropriate tasks
- [ ] Description is specific enough for auto-selection
- [ ] Agent does NOT activate for out-of-scope tasks

**Explicit Invocation**:
- [ ] Agent responds when directly named
- [ ] Agent follows system prompt instructions
- [ ] Agent uses only permitted tools

**Context Isolation**:
- [ ] Agent maintains separate context from main conversation
- [ ] Agent doesn't pollute main conversation context
- [ ] Agent can run in parallel with others without interference

**Output Quality**:
- [ ] Output matches specified format
- [ ] Reasoning is clear and thorough
- [ ] Recommendations are specific and actionable
- [ ] Output includes all required sections

**Token Efficiency**:
- [ ] System prompt is concise (avoid fluff)
- [ ] Uses references instead of copying content
- [ ] Doesn't request unnecessary file reads

**Tool Usage**:
- [ ] Only uses permitted tools
- [ ] Uses tools appropriately (parallel when possible)
- [ ] Handles tool errors gracefully

### 10.3 Common Issues & Solutions

**Issue**: Agent activates inconsistently

**Solutions**:
- Make description more specific and action-oriented
- Include trigger phrases: "Use for [explicit scenarios]"
- Name agent explicitly in workflow if auto-selection unreliable

---

**Issue**: Agent produces verbose auto-generated responses

**Solutions**:
- Trim system prompt to essentials
- Use references to docs instead of copying content
- Add explicit output format requirements
- Remove generic fluff language

---

**Issue**: Agent returns shallow output despite detailed prompt

**Solutions**:
- Add "think hard" or "think harder" to request
- Include output length requirements
- Provide examples of desired depth
- Add checklist items that require thorough analysis

---

**Issue**: Agent ignores important instructions

**Solutions**:
- Use emphasis: "IMPORTANT:", "YOU MUST:", "NEVER:"
- Place critical instructions early in prompt
- Repeat critical constraints in multiple sections
- Add verification checkboxes

---

**Issue**: Agent uses wrong tools or exceeds permissions

**Solutions**:
- Explicitly list allowed tools in frontmatter
- Add tool usage policy section with restrictions
- Provide examples of correct tool usage
- Use hooks to enforce tool restrictions

---

## 11. Packaging, Sharing & Plugin Distribution

### 11.1 Plugin Structure

**Directory Layout**:
```
my-plugin/
‚îú‚îÄ‚îÄ .claude-plugin/
‚îÇ   ‚îî‚îÄ‚îÄ plugin.json         # Plugin manifest
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ code-reviewer.md
‚îÇ   ‚îú‚îÄ‚îÄ security-auditor.md
‚îÇ   ‚îî‚îÄ‚îÄ deploy-agent.md
‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îú‚îÄ‚îÄ review.md
‚îÇ   ‚îú‚îÄ‚îÄ deploy.md
‚îÇ   ‚îî‚îÄ‚îÄ test.md
‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îî‚îÄ‚îÄ hooks.json
‚îú‚îÄ‚îÄ .mcp.json              # Optional MCP servers
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ LICENSE
```

**Plugin Manifest** (`.claude-plugin/plugin.json`):
```json
{
  "name": "dev-workflow-suite",
  "version": "1.0.0",
  "description": "Production-ready agents for development workflow",
  "author": {
    "name": "Your Team",
    "email": "team@example.com"
  },
  "keywords": ["development", "code-review", "security", "deployment"],
  "homepage": "https://github.com/yourorg/dev-workflow-suite",
  "repository": "https://github.com/yourorg/dev-workflow-suite",
  "license": "MIT"
}
```

### 11.2 Enable Only When Needed

**Principle**: Ship agents + hooks + commands as plugins; enable selectively to reduce prompt bloat.

**Usage**:
```bash
# List available plugins
/plugin list

# Enable specific plugin
/plugin enable dev-workflow-suite@company-marketplace

# Disable when not needed
/plugin disable dev-workflow-suite@company-marketplace
```

**Benefits**:
- Reduced token consumption (only active plugins loaded)
- Team sharing via plugin marketplace
- Version control for agent updates
- Easy enable/disable for different projects

### 11.3 Version Control & Team Sharing

**What to Check Into Git**:
```
‚úÖ .claude/agents/*.md        # Subagent definitions
‚úÖ .claude/commands/*.md       # Slash commands
‚úÖ .claude/settings.json       # Shared permissions
‚úÖ .claude/hooks/hooks.json    # Hook configuration
‚úÖ CLAUDE.md                   # Project context
‚úÖ README.md                   # Plugin documentation

‚ùå .claude/settings.local.json  # Personal settings
‚ùå .env, secrets/**            # Never commit secrets
```

**Collaborative Workflow**:
1. Check agents into version control
2. Team members pull changes automatically
3. Team improves agents collaboratively
4. Changes reviewed in pull requests
5. Version bumped for breaking changes

**Documentation Requirements** (README.md):
```markdown
# Plugin Name

## Description
[What this plugin does]

## Agents Included
- **agent-name**: [What it does, when to use]
- ...

## Commands Included
- `/command-name`: [What it does]
- ...

## Installation
```bash
/plugin marketplace add yourorg/plugins
/plugin install plugin-name@marketplace
```

## Configuration
[Any required setup]

## Usage Examples
[Common scenarios]

## Troubleshooting
[Common issues and solutions]
```

---

## 12. Advanced Patterns & Orchestration

### 12.1 Sequential Workflow Pattern

**Use Case**: Multi-phase tasks requiring ordered execution.

**Example**:
```markdown
requirements-analyst ‚Üí system-architect ‚Üí code-reviewer
```

**Implementation**:
```markdown
---
name: project-orchestrator
description: Orchestrates multi-phase development workflows
tools: Task
model: sonnet
---

## SEQUENTIAL WORKFLOW

For end-to-end feature development:

### Phase 1: Requirements Analysis
Invoke `requirements-analyst` agent:
- Analyze feature request
- Identify acceptance criteria
- Document requirements in requirements.md

CHECKPOINT: Requirements approved before proceeding

### Phase 2: System Design
Invoke `system-architect` agent:
- Design system components
- Create API contracts
- Document architecture decisions
- Output: architecture.md

CHECKPOINT: Design reviewed before implementation

### Phase 3: Code Review Setup
Invoke `code-reviewer` agent:
- Review architecture for quality standards
- Identify potential issues
- Suggest improvements

OUTPUT: Ready-to-implement specification in design-doc.md
```

### 12.2 Parallel Execution Pattern

**Use Case**: Independent tasks that can run simultaneously.

**Example**:
```markdown
ui-engineer + api-designer + database-schema-designer
```

**Implementation**:
```markdown
---
name: fullstack-builder
description: Builds full-stack features with parallel frontend/backend/database work
tools: Task
model: sonnet
---

## PARALLEL EXECUTION

For full-stack feature development:

### Concurrent Tasks (All Start Together)

**Task 1: Frontend Development** (ui-engineer agent)
- Build React components
- Implement state management
- Create tests
- Output: UI implementation

**Task 2: API Development** (api-designer agent)
- Design API endpoints
- Implement controllers
- Add validation
- Output: API implementation

**Task 3: Database Design** (database-schema-designer agent)
- Design schema
- Create migrations
- Add indexes
- Output: Database layer

CHECKPOINT: All three complete before integration

### Integration Phase
After parallel tasks complete:
- Integrate frontend with API
- Test end-to-end flows
- Deploy to staging
```

### 12.3 Intelligent Routing Pattern

**Use Case**: Dynamic delegation based on task analysis.

**Implementation**:
```markdown
---
name: smart-router
description: Analyzes tasks and routes to appropriate specialist agent
tools: Task, Read, Grep
model: sonnet
---

## INTELLIGENT ROUTING

### Phase 1: Task Analysis
1. Read task description
2. Identify task type and domain
3. Assess complexity and risk level

### Phase 2: Agent Selection

**Security-Related** (keywords: auth, password, vulnerability, security, encryption)
‚Üí Route to `security-auditor` agent

**Performance Issues** (keywords: slow, performance, optimization, latency, bottleneck)
‚Üí Route to `performance-optimizer` agent

**API Design** (keywords: endpoint, REST, API, GraphQL, route)
‚Üí Route to `api-designer` agent

**Database Work** (keywords: schema, migration, query, database, SQL)
‚Üí Route to `database-architect` agent

**Frontend** (keywords: component, UI, React, Vue, CSS, responsive)
‚Üí Route to `ui-engineer` agent

**Testing** (keywords: test, coverage, QA, assertion, spec)
‚Üí Route to `test-engineer` agent

**General Code Review** (default if no specific match)
‚Üí Route to `code-reviewer` agent

### Phase 3: Delegation
Invoke selected specialist agent with full context
```

### 12.4 Cross-Model Review Pattern

**Use Case**: High-stakes decisions requiring multi-model validation.

**Implementation**:

**Step 1**: Create MCP tool that forwards request to multiple models

**Step 2**: System prompt with review workflow
```markdown
---
name: critical-reviewer
description: Multi-model review for critical changes (security, data migrations, major refactors)
tools: Read, mcp__multi_model_review
model: opus
---

## CRITICAL REVIEW PROCESS

For high-risk changes:

### Phase 1: Initial Review (Opus)
Conduct thorough analysis using extended thinking

### Phase 2: Cross-Validation
Invoke multi-model review tool:
- Submit code to Opus, Sonnet, and external model
- Collect verdicts (pass/warn/block)
- Aggregate results

### Phase 3: Consensus Analysis
- **All Pass**: Approve with confidence
- **Mixed Results**: Deep dive into disagreements, resolve discrepancies
- **Any Block**: Detailed investigation required, do not proceed

### Phase 4: Documentation
Document review results:
- Models consulted
- Concerns raised
- Resolution approach
- Final verdict with rationale
```

### 12.5 Human-in-the-Loop Pattern

**Use Case**: Approval gates for destructive or high-risk operations.

**Implementation** (using hooks):

**Hook Configuration** (`hooks/hooks.json`):
```json
{
  "hooks": [
    {
      "event": "PreToolUse",
      "command": "./scripts/approval-check.sh",
      "description": "Check for pending approvals before tool use",
      "blocking": true
    }
  ]
}
```

**Approval Script** (`scripts/approval-check.sh`):
```bash
#!/bin/bash
# Check for approval queue file

QUEUE_FILE=".claude/approval-queue.txt"

if [ -f "$QUEUE_FILE" ]; then
  echo "‚è∏Ô∏è  APPROVAL REQUIRED"
  cat "$QUEUE_FILE"
  echo ""
  echo "Next command: $(cat $QUEUE_FILE | tail -n1)"
  echo "Run: echo 'approved' > .claude/approval-response.txt"
  exit 1
fi

# Check if approval was granted
if [ -f ".claude/approval-response.txt" ]; then
  if grep -q "approved" ".claude/approval-response.txt"; then
    rm .claude/approval-response.txt
    exit 0
  else
    echo "‚ùå Approval denied"
    exit 1
  fi
fi

exit 0
```

**Agent System Prompt**:
```markdown
For destructive operations:
1. Write details to .claude/approval-queue.txt:
   - Operation description
   - Affected resources
   - Risk level
   - Proposed command
2. Wait for human to approve (approval-response.txt)
3. Execute only after approval
4. Clean up queue file after execution
```

---

## 13. Specialized Agent Categories

### 13.1 Essential Starter Set

**Minimum Viable Agent Configuration** (3-5 agents):

1. **code-reviewer** - Quality gate for all changes
2. **security-auditor** - Security compliance
3. **test-suite-generator** - Testing automation
4. **[language]-specialist** - Language-specific expertise (Python/TypeScript/etc.)
5. **performance-optimizer** - Optional but recommended

**Benefits**: Covers 80% of common needs with minimal overhead

### 13.2 Language Specialists

**Python Backend Engineer**:
```markdown
---
name: python-backend-engineer
description: Expert Python developer for Django/FastAPI/Flask backend development
tools: Read, Grep, Glob, Edit, Bash
model: sonnet
---
Specializes in:
- Django ORM and migrations
- FastAPI async patterns
- Flask application factory
- Celery task queues
- pytest testing strategies
- Python packaging and dependencies
```

**Backend TypeScript Architect**:
```markdown
---
name: backend-typescript-architect
description: Expert TypeScript backend developer for Node.js/Express/NestJS
tools: Read, Grep, Glob, Edit, Bash
model: sonnet
---
Specializes in:
- Express middleware patterns
- NestJS modules and dependency injection
- TypeORM/Prisma database access
- JWT authentication
- Jest/Supertest testing
- TypeScript advanced types
```

**UI Engineer**:
```markdown
---
name: ui-engineer
description: Frontend specialist for React/Vue/Angular component development
tools: Read, Grep, Glob, Edit, Bash
model: sonnet
---
Specializes in:
- React hooks and component patterns
- State management (Redux/Zustand/Context)
- CSS-in-JS and Tailwind
- Accessibility (ARIA, WCAG)
- Testing Library and Playwright
- Performance optimization
```

### 13.3 Quality & Security Specialists

**Test Suite Generator**:
```markdown
---
name: test-suite-generator
description: Generates comprehensive test suites with unit, integration, and e2e tests
tools: Read, Grep, Glob, Edit, Write, Bash
model: sonnet
---
Specializes in:
- Test-driven development
- Unit test generation
- Integration test scenarios
- E2E user flows
- Mock and stub strategies
- Coverage analysis
```

**Code Refactoring Specialist**:
```markdown
---
name: refactoring-specialist
description: Refactors legacy code for improved maintainability, performance, and testability
tools: Read, Grep, Glob, Edit, Bash
model: sonnet
---
Specializes in:
- Legacy code modernization
- Design pattern application
- Complexity reduction
- Dependency extraction
- Test coverage improvement
- Documentation updates
```

**Penetration Tester**:
```markdown
---
name: penetration-tester
description: Simulates attacks to identify vulnerabilities before malicious actors do
tools: Read, Grep, Glob, Bash
model: opus
---
Specializes in:
- Attack surface enumeration
- Exploitation scenario development
- Security control bypass
- Privilege escalation paths
- Data exfiltration vectors
- Remediation priority
```

---

## 14. MCP Integration & Prompt Hygiene

### 14.1 MCP Prompt Templates

**Location**: Defined in MCP server implementation

**Best Practices**:
- Validate prompt arguments before processing
- Version templates for compatibility
- Document expected inputs/outputs
- Compose prompts modularly

**Example MCP Prompt Template**:
```json
{
  "prompts": [
    {
      "name": "code-review",
      "description": "Review code changes for quality and security",
      "arguments": [
        {
          "name": "file_path",
          "description": "Path to file to review",
          "required": true
        },
        {
          "name": "severity",
          "description": "Minimum severity to report (low|medium|high|critical)",
          "required": false,
          "default": "medium"
        }
      ]
    }
  ]
}
```

**Agent Integration**:
```markdown
---
name: mcp-code-reviewer
description: Code reviewer using MCP review tool
tools: mcp__code_review
model: sonnet
---

To review code:
1. Invoke mcp__code_review tool with:
   - file_path: path to file
   - severity: "medium" (or higher for stricter review)
2. Parse results and format for user
3. Provide actionable recommendations
```

### 14.2 Modular Prompt Composition

**Pattern**: Break complex prompts into reusable components.

**Base Template** (`.claude/prompts/base-agent.md`):
```markdown
You are a specialized AI agent with:
- Clear purpose and scope
- Tool usage policies
- Output format requirements
- Safety constraints
```

**Extension Template** (agent-specific):
```markdown
---
name: custom-agent
description: Custom agent description
---

{{include: base-agent.md}}

## SPECIFIC CAPABILITIES
[Agent-specific content]
```

---

## 15. Critical Success Factors Summary

### What Works ‚úÖ

1. **Separation of Concerns**
   - One responsibility per agent
   - Clear domain boundaries
   - Independent execution contexts

2. **Concrete Examples**
   - Provide 2-5 diverse examples
   - Show both good and bad patterns
   - Include expected outputs

3. **Progressive Capability Expansion**
   - Start with minimal tool set
   - Validate behavior before expanding
   - Test each capability addition

4. **Explicit Instructions**
   - Use direct, imperative language
   - Emphasize critical rules (IMPORTANT, YOU MUST)
   - Provide step-by-step workflows

5. **Validation Checkpoints**
   - Definition of Done checklists
   - Verification commands
   - Success criteria

### What Fails ‚ùå

1. **Super-Agent Anti-Pattern**
   - Trying to create one agent for everything
   - Mixing multiple responsibilities
   - Vague, general capabilities

2. **Vague Activation Descriptions**
   - Generic: "Help with coding tasks"
   - Better: "Use for RESTful API design with OpenAPI specs"

3. **Excessive Context**
   - Copying entire files into prompts
   - Redundant explanations
   - Verbose auto-generated content

4. **Missing Error Handling**
   - No failure recovery steps
   - Unclear error messages
   - No rollback procedures

5. **No Clear Success Criteria**
   - Undefined "done" state
   - No verification steps
   - Ambiguous quality standards

---

## 16. Quick Start Checklist

### For Your First Agent

- [ ] **Choose single responsibility** (code review, testing, deployment, etc.)
- [ ] **Start with template** from section 9 (Production-Ready Templates)
- [ ] **Customize role definition** to match your project domain
- [ ] **Add project context** by creating/updating CLAUDE.md
- [ ] **Limit tools** to minimum needed (start with Read, Grep, Glob)
- [ ] **Write clear description** for auto-activation (action-oriented)
- [ ] **Include 2-3 examples** of expected behavior
- [ ] **Add anti-patterns section** listing common mistakes
- [ ] **Define output format** with explicit structure
- [ ] **Create Definition of Done** checklist
- [ ] **Test on real task** and observe behavior
- [ ] **Refine based on results** (iterate 2-3 times)
- [ ] **Document activation criteria** in README
- [ ] **Share with team** via git or plugin

### For Agent Workflows

- [ ] **Identify workflow type** (sequential, parallel, or routing)
- [ ] **Map dependencies** between agents
- [ ] **Define handoff points** and data formats
- [ ] **Add approval gates** for high-risk operations
- [ ] **Create orchestrator agent** if needed
- [ ] **Test end-to-end flow** with real scenarios
- [ ] **Document workflow** with diagrams and examples

### For Production Deployment

- [ ] **Set up CLAUDE.md** with project-specific context
- [ ] **Configure permissions** in settings.json (deny dangerous operations)
- [ ] **Add hooks** for quality gates (lint, test, security scan)
- [ ] **Create slash commands** for common workflows
- [ ] **Package as plugin** for easy distribution
- [ ] **Document for team** with examples and troubleshooting
- [ ] **Version control** all agent files
- [ ] **Establish review process** for agent changes

---

## Conclusion

Effective Claude Code agent system prompts require:

1. **Clear architecture** - Single responsibility, appropriate altitude, minimal context
2. **Structured format** - YAML frontmatter + organized sections (role, capabilities, workflow, examples, anti-patterns)
3. **Progressive refinement** - Start minimal, test on real tasks, expand based on performance
4. **Tool discipline** - Least privilege, progressive expansion, explicit usage policies
5. **Validation focus** - Definition of Done, verification steps, success criteria
6. **Safety first** - Permission controls, secret management, human approval gates
7. **Team collaboration** - Version control, documentation, plugin distribution

Start with the Essential Starter Set (3-5 agents), validate on real work, then expand strategically based on workflow bottlenecks. Use production-ready templates from section 9, customize for your domain, and iterate based on observed behavior.

The most successful agents combine clear purpose, concrete examples, minimal tool scope, and iterative refinement. Follow this guide to create reliable, autonomous AI assistants that enhance rather than disrupt your development workflow.

---

*Guide synthesized from Claude Code official documentation, Anthropic engineering guides, Model Context Protocol specifications, and community best practices (2024-2025).*
